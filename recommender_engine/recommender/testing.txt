

1. Evaluate the Pattern (Taste) the User Has

How to Test:


Hold-out Set: Split user interaction history. Train the recommender on one part (training set) and test its ability to predict items the user interacted with in the other part (test set).

          Metrics:
          
          Precision@k: Out of the top K recommendations shown, what fraction did the user actually interact with in the test set? (Measures exactness).
          
          Recall@k: Out of all the items the user interacted with in the test set, what fraction were captured in the top K recommendations? (Measures completeness).
          
          NDCG@k (Normalized Discounted Cumulative Gain): Measures the ranking quality. Did the engine rank the truly relevant items higher in the list of K recommendations? (Considers both relevance and position).
          
          MAP (Mean Average Precision): Similar to NDCG, evaluates the quality of ranking across different list lengths.

          
    Accuracy Metrics

Precision: Proportion of recommended items that are relevant
Recall: Proportion of relevant items that are recommended
F1-Score: Harmonic mean of precision and recall
Mean Average Precision (MAP): Average precision across all recommendations
Normalized Discounted Cumulative Gain (NDCG): Measures ranking quality with position-based discounting     


2. Variation of the Recommendations (Don't Make the Rich Richer)

How to Test:

Intra-List Diversity: Measure how different the items are within a single recommendation list for a user.

Metrics: Calculate average pairwise dissimilarity between recommended items (using item features, embeddings, or category information). Higher dissimilarity = more diverse list.

Aggregate Diversity (Across Users): Measure the diversity of items recommended across all users over a period.

       Metrics:
       
       Gini Coefficient / Shannon Entropy: Applied to the distribution of recommended item counts. A lower Gini or higher Entropy indicates recommendations are spread across more items rather than concentrated on a few popular ones.
       
       Popularity Bias Analysis: Compare the average popularity rank/score of recommended items versus the average popularity rank/score of items users organically interact with. Are recommended items significantly more popular?
       
       Track Recommendation Frequency: Monitor how often individual items get recommended. Are a small number of items dominating the recommendations?
       
3. Serendipity ('Surprise Me') / Exploit-Explore Testing



How to Test:

Defining Serendipity Metrics (Challenging): Serendipity requires both unexpectedness and relevance/utility.

Unexpectedness: How surprising is the recommendation? Could be measured by how dissimilar it is to the user's typical interactions or how unlikely it was predicted by a simpler baseline model (e.g., popularity, basic collaborative filter).

Relevance/Utility: Did the user actually find the unexpected item useful (e.g., clicked, purchased, rated highly)?

Combined Metric: Some research combines these, e.g., calculate the proportion of recommendations that were both unexpected (above a threshold) and positively interacted with.

Novelty: Measure how many recommended items are new to the user (never seen/interacted with before). High novelty is often a prerequisite for serendipity.

Exploration Metrics (if applicable): If using algorithms like Multi-Armed Bandits or reinforcement learning, track the proportion of recommendations allocated to exploration vs. exploitation.

Qualitative Evaluation:

User Surveys: Ask users: "Did these recommendations help you discover something new and interesting?"

Analyze Interaction Patterns: Look for cases where a user interacted positively with an item far from their usual taste profile after it was recommended.

4. Coverage


How to Test:

Catalog Coverage Metric: (Number of unique items recommended across all users in a given period) / (Total number of items eligible for recommendation in the catalog)

Define "Eligible Items": Be clear about the denominator. Are you including out-of-stock items? Items in irrelevant categories? Define the pool of items you could potentially recommend.

Segmented Coverage: Analyze coverage within specific segments:
<!-- 
       Coverage of newly added items.
       
       Coverage of long-tail items (low popularity).
       
       Coverage by category or price range. Are certain segments under-represented in recommendations?
       
       User Coverage: A related metric is the percentage of active users who actually receive any recommendations. -->

